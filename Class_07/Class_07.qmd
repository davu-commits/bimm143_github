---
title: "Class 7: Machine Learning 1"
author: "Dan Vu (PID: A17380158"
format: pdf
---

Today we will explore some fundamental machine learning methods including clustering and dimensionality reduction.

## K-means clustering

To see how this works let's first makeup some data to cluster where we know what the answer should be. We can use `rnorm()` function to help here:

```{r}
x <- c(rnorm(30, mean = -3), rnorm(30, mean = 3))
y <- rev(x)
```


```{r}
x <- cbind(x,y)
k <- kmeans(x, centers = 3)
plot(x, col = k$cluster, pch = 16)
points(k$centers, col = "blue", pch = 15, cex = 2)
```

The function for K-means clustering in "base" R is `kmeans()`

```{r}
k <- kmeans(x, centers = 2)
k
```

To get the results of the return list object we can use the dollar `$` syntax.

>Q. How many points are in each cluster?

```{r}
k$size
```

>Q. How many points are in each cluster?
  - cluster assignment/membership?
  - cluster center?
  

>Q. Make a clustering results figure of the data colored by the cluster membership and show cluster centers.

```{r}
x <- c(rnorm(30, mean = -3), rnorm(30, mean = 3))
y <- rev(x)
```
```{r}
x <- cbind(x,y)
plot(x, col=k$cluster, pch = 16)
points(k$centers, col="blue", pch = 15, cex=2)
```

K-means clustering is very popular as it is very fast and relatively straightforward; it takes numeric data as input and returns the cluster membership vector etc. 

The "issue" is we tell `kmeans()` how many clusters we want!

>Q. Run kmeans again and cluster into 4 groups and plot the results.

```{r}
k4 <- kmeans(x, centers =4)
plot(x, col=k4$cluster)
points(k4$centers, pch=15)
```

```{r}
results <- list()
for (k in 1:5) {
  results[[as.character(k)]] <- kmeans(x, centers = k)
}
for (k in 1:5) {
  plot(x, col = results[[as.character(k)]]$cluster)
  points(results[[as.character(k)]]$centers, pch = 15)
}
ss <- sapply(results, function(res) res$tot.withinss)
plot(1:5, ss, type = "b", xlab = "Number of clusters (k)", ylab = "Total within SS")
```

##Hierarchical CLustering

THe mean "base" R function for Hierarchical Clustering is called `hclust()`. Here we can't just input our data we need to first calculate a distance matrix (e.g. `dist()`) for our data and use this as input to `hclust()`. 
```{r}
d <-dist(x)
hc <- hclust(d)
hc
```

There is a plot method for hclust results lets try it.

```{r}
plot(hc)
abline(h=8, col="red")
```

To get our cluster "membership" vector (i.e. our main clustering result) we can "cut" the tree at a given height or at a height that yields a given "k" groups.

```{r}
grps <- cutree(hc, h=8)
```

>Q. Plot the data with our hclust result coloring.

```{r}
plot(x, col = grps)
```

# Principal Component Analysis (PCA)

## PCA of UK Food Data

Import food data from an online CSV file: 

```{r}
x <- read.csv("~/Downloads/UK_foods.csv")
head(x)
```

```{r}
rownames(x) <-  x[,1]
x <- x[,-1]
x
```

```{r}
x <- read.csv("~/Downloads/UK_foods.csv", row.names=1)
x
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

There is one plot that can be useful for small datasets:

```{r}
pairs(x, col=rainbow(10), pch=16)
```

> Main point: it can be difficult to spot major trends and patterns even in relatively small multivariate database.

##This is how PCA can help us

The main function in "base" R for PCA is called `prcomp()`

I will take the transpose (`t()`) of our food data so the "foods" are in the columns:
```{r}
PCA <- prcomp(t(x))
summary (PCA)
```

```{r}
PCA$x
cols <- c("orange", "red", "blue", "green")
plot(PCA$x[,1], PCA$x[,2], col = cols, pch = 16)
```

```{r}
library(ggplot2)
```

```{r}
ggplot(PCA$x) +
  aes(PC1, PC2) +
  geom_point(col=cols)
```

```{r}
ggplot(PCA$rotation) +
  aes(PC1, rownames(PCA$rotation)) +
  geom_col()
```

PCA looks very useful for viewing data. 